{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHALLENGE: GLOBAL TERORISM ATTACK. Data provided from the website: https://www.start.umd.edu/gtd/\n",
    "#NOTE: the downloaded data should be saved with the extension .csv\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# Steps to perform the task:\n",
    "#     1.Data gathering: importing the data\n",
    "#     2.Data pre-processing: perform data cleaning (e.g. incomplete data),attribute selection, etc\n",
    "#     3.Data mining: split the data into training and test data, apply data mining techniques on the test data\n",
    "#     4.Perform evaluation of the data mining techniques (e.g. accuracy of the data technique)\n",
    "#=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import file\n",
    "data_terr_attack = pd.read_csv(\"C:/Users/Milos Milojevic/Documents/Python Scripts/gtd_1970_1994.csv\",encoding='ISO-8859-1',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove rows of the dataframe for which class attribute (gname) is not defined\n",
    "data_terr_attack_90 = data_terr_attack[data_terr_attack.gname != \"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOTE: Reduce the amount of data stored in a dataframe since there is no enough RAM memory to process all data on the used laptop  \n",
    "data_terr_attack_90 = data_terr_attack_90[data_terr_attack_90.iyear >= 1991]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit long strings\n",
    "def shortening(df):\n",
    "    \n",
    "    df.loc[df['weaptype1_txt'] == 'Vehicle (not to include vehicle-borne explosives, i.e., car or truck bombs)', 'weaptype1_txt'] = 'Vehicle'\n",
    "    df.loc[df['propextent_txt'] == 'Minor (likely < $1 million)','propextent_txt'] = 'Minor (< $1 million)'\n",
    "    df.loc[df['propextent_txt'] == 'Major (likely > $1 million but < $1 billion)','propextent_txt'] = 'Major (< $1 billion)'\n",
    "    df.loc[df['propextent_txt'] == 'Catastrophic (likely > $1 billion)', 'propextent_txt'] = 'Catastrophic (> $1 billion)'\n",
    "\n",
    "    return df\n",
    "data_terr_attack_90 = shortening(data_terr_attack_90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Values -9 are unknowns replaced with nan\n",
    "def nines(df):\n",
    "    for i in [\"ishostkid\", \"property\",\"INT_LOG\",\"INT_IDEO\",\"INT_MISC\",\"INT_ANY\",\"claimed\"]:\n",
    "         df.loc[df[i] == -9,i]=df.loc[df[i] == -9,i].replace(-9,np.nan)\n",
    "    return df  \n",
    "data_terr_attack_90 = nines(data_terr_attack_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Values -99 are unknowns replaced with nan\n",
    "def ninty_nines(df):\n",
    "    for i in [\"nhostkid\",\"nhostkidus\",\"nhours\",\"nperpcap\",\"nperps\"]: \n",
    "        df.loc[df[i] == -99,i]=df.loc[df[i] == -99,i].replace(-99,np.nan)\n",
    "    return df  \n",
    "data_terr_attack_90 = ninty_nines(data_terr_attack_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eventid                  0\n",
      "iyear                    0\n",
      "imonth                   0\n",
      "iday                     0\n",
      "approxdate            8539\n",
      "extended                 0\n",
      "resolution            8391\n",
      "country                  0\n",
      "country_txt              0\n",
      "region                   0\n",
      "region_txt               0\n",
      "provstate             1621\n",
      "city                     0\n",
      "latitude               538\n",
      "longitude              538\n",
      "specificity              0\n",
      "vicinity                 0\n",
      "location              8492\n",
      "summary               8453\n",
      "crit1                    0\n",
      "crit2                    0\n",
      "crit3                    0\n",
      "doubtterr                0\n",
      "alternative           7029\n",
      "alternative_txt       7029\n",
      "multiple                 0\n",
      "success                  0\n",
      "suicide                  0\n",
      "attacktype1              0\n",
      "attacktype1_txt          0\n",
      "                      ... \n",
      "propextent            7663\n",
      "propextent_txt        7663\n",
      "propvalue             7682\n",
      "propcomment           8437\n",
      "ishostkid                1\n",
      "nhostkid              8195\n",
      "nhostkidus            8175\n",
      "nhours                8386\n",
      "ndays                 8400\n",
      "divert                8529\n",
      "kidhijcountry         8248\n",
      "ransom                  10\n",
      "ransomamt             8527\n",
      "ransomamtus           8541\n",
      "ransompaid            8537\n",
      "ransompaidus          8541\n",
      "ransomnote            8541\n",
      "hostkidoutcome        8373\n",
      "hostkidoutcome_txt    8373\n",
      "nreleased             8405\n",
      "addnotes              8463\n",
      "scite1                8453\n",
      "scite2                8454\n",
      "scite3                8469\n",
      "dbsource                 0\n",
      "INT_LOG               1338\n",
      "INT_IDEO              1357\n",
      "INT_MISC                37\n",
      "INT_ANY               1152\n",
      "related               7267\n",
      "Length: 135, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking the number of nan values per column, in order to select appropriate atributes\n",
    "feature_null_counts = data_terr_attack_90.apply(lambda col: col.isnull().sum(), axis = 0)\n",
    "print (feature_null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since the observed dataframe is composed of text attributes and numeric attributes, the pre-processing step for these attributes is diferent. \n",
    "#Thus, attributes are first separated into text attributes and the numeric attributes. By observing the values of 'feature_null_counts', attrbibutes\n",
    "#,both numeric and text attributes, are selected only in case there is a significant number of non nan values. Additionally some of the attributes like for example\n",
    "#'targettyp1' and 'targettype1_txt' are saying the same thing in different form, thus only one of them is choosen \n",
    "att_num = ['iyear','imonth','iday','extended','country','region','specificity','vicinity','crit1','crit2','crit3','doubtterr','alternative','multiple','success','suicide','attacktype1','targtype1','targsubtype1','natlty1',\n",
    "            'guncertain1','individual','nperps','weaptype1','weapsubtype1','nkill','nkillus','nkillter','nwound','nwoundus','nwoundte','property','propextent','propvalue','ishostkid','ransom','INT_LOG','INT_IDEO','INT_MISC','INT_ANY']\n",
    "\n",
    "att_text = ['provstate','city','location','corp1','target1','dbsource','weapdetail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a class (gname)\n",
    "class_gname = data_terr_attack_90.gname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define observed dataframe\n",
    "data_terr_attack_90 = data_terr_attack_90[att_num+att_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use 'train_test_split' to split the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_terr_attack_90, class_gname, test_size = 0.33, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an empty list in which different pipelines will be stored\n",
    "pipelines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function that processes the numeric attributes\n",
    "def process_att_num(df):\n",
    "    df = df[att_num]\n",
    "    \n",
    "    #Select columns with numeric attributes\n",
    "    numeric_col = ['iyear','imonth','iday','nperps','nkill','nkillus','nkillter','nwound','nwoundus','nwoundte','propvalue']\n",
    "   \n",
    "    #Replace nulls with medains\n",
    "    for i in numeric_col:\n",
    "        df.ix[df[i].isnull(), i] = df[i].median()\n",
    "   \n",
    "    #Seclet binary columns\n",
    "    binary_col = ['extended','country','region','specificity','vicinity','crit1','crit2','crit3','doubtterr','alternative','multiple','success','suicide','attacktype1','targtype1',\n",
    "                  'targsubtype1','natlty1','guncertain1','individual','weaptype1','weapsubtype1','property','propextent','ishostkid','ransom','INT_LOG','INT_IDEO','INT_MISC','INT_ANY']\n",
    "     \n",
    "    #Replace null values with a radnom sample\n",
    "    for i in binary_col:\n",
    "        df.loc[df[i].isnull(), i] = np.random.choice([0,1],size = df[i].isnull().sum())       \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function for text attributes that fills all nan values with '' \n",
    "def process_att_text(col):\n",
    "    \n",
    "    def nest_function(df):\n",
    "\n",
    "        return df[col].fillna(\"\")\n",
    "    \n",
    "    return nest_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a Document Term Matrix using CountVectorizer for all text attributes defined in att_text list\n",
    "for i in att_text:\n",
    "    #FunctionTransformer turns any function into a transformer\n",
    "    pipeline = make_pipeline(FunctionTransformer(process_att_text(i),validate = False),CountVectorizer(decode_error = 'ignore'))\n",
    "    #Add pipeline to pipelines list\n",
    "    pipelines.append(pipeline)\n",
    "\n",
    "#Add the numeric data into the pipeline\n",
    "pipelines.append(FunctionTransformer(process_att_num, validate = False))\n",
    "\n",
    "#Make union of the pipelines\n",
    "union = make_union(*pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform the training data\n",
    "X_train_new = union.fit_transform(X_train)\n",
    "#Transform test data\n",
    "X_test_new = union.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# #Removing features (attributes) with low variance  \n",
    "#==============================================================================\n",
    "#VarianceThreshold removes all features whose variance doesn’t meet a predefined threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# Set threshold to 0.1\n",
    "sel = VarianceThreshold(threshold = 0.1)\n",
    "sel.fit(X_train_new.toarray())\n",
    "X_train_update=sel.transform(X_train_new)\n",
    "# Subset features\n",
    "X_test_update=sel.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "from sklearn.grid_search import GridSearchCV #for tuning the hyper-parameters of the estimator\n",
    "from sklearn.metrics import accuracy_score #for evaluating the accuracy of the classifier\n",
    "from sklearn.model_selection import cross_val_score #for evaluating a score by cross-validation\n",
    "#Create an empty dictionary for storing the accuracy scores from different data mining techniques\n",
    "accuracy_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:552: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# 1.K-nearest neighbour (KNN) Classifier \n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "#==============================================================================\n",
    "#Import packages \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#Check parameters of the classifier and their default values\n",
    "KNeighborsClassifier().get_params\n",
    "#Perform tuning hyper-parameters of the etsimator using GridSearchCV\n",
    "n_list = list(range(3, 30))\n",
    "#Perform GridSearchCV for classifier\n",
    "knn_gscv= GridSearchCV(KNeighborsClassifier(), param_grid = {'n_neighbors':n_list})\n",
    "#Perform fit method\n",
    "knn_gscv.fit(X_train_update, y_train)\n",
    "#Check the best parameter\n",
    "print (knn_gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3) \n",
    "#Fit the model using training data X_train_update and y_train as target values\n",
    "knn_model.fit(X_train_update,y_train)\n",
    "#Predict the class labels for the test data X_test_update\n",
    "knn_predict = knn_model.predict(X_test_update)\n",
    "#Check accuracy classifications score\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "#and store it in the accuracy dictionary accuracy_dict\n",
    "accuracy_dict['knn'] = accuracy_score(y_test, knn_predict)\n",
    "#Perfrom cross-validation\n",
    "knn_score = cross_val_score(knn_model, X_test_update, y_test, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# 2. Naive (Gaussian) Bayesian Classifier \n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB\n",
    "#==============================================================================\n",
    "#Import packages \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Create an instance of the model\n",
    "naive_bay_model = GaussianNB()\n",
    "#Fit the model using training data X_train_update and y_train as target values\n",
    "naive_bay_model.fit(X_train_update.toarray(),y_train) #In order to avoid getting the \"TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\" we use X_train_dtm and we write X_train_dtm.toarray()\n",
    "#Predict the class labels for the test data X_test_update\n",
    "naive_bay_predict = naive_bay_model.predict(X_test_update.toarray())\n",
    "#Check accuracy classifications and store it in the accuracy dictionary accuracy_dict\n",
    "accuracy_dict['naive_bay'] = accuracy_score(y_test, naive_bay_predict)\n",
    "#Perfrom cross-validation\n",
    "naive_bay_score = cross_val_score(naive_bay_model, X_test_update.toarray(), y_test, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:552: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# 3.Decision Tree Classifier \n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "#====================================================== ========================\n",
    "#Import packages\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#Check parameters of the classifier\n",
    "DecisionTreeClassifier().get_params\n",
    "#Perform GridSearchCV for classifier\n",
    "dec_tr_gscv = GridSearchCV(DecisionTreeClassifier(),param_grid = {'criterion': ['gini','entropy']})\n",
    "#Fit the model using training data X_train_update and y_train as target values\n",
    "dec_tr_gscv.fit(X_train_update, y_train)\n",
    "#Check the best parameter\n",
    "print (dec_tr_gscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the model\n",
    "dec_tr_model = DecisionTreeClassifier(criterion='entropy') \n",
    "#Fit the model using training data X_train_update and y_train as target values\n",
    "dec_tr_model.fit(X_train_update,y_train)\n",
    "#Predict the class labels for the test data X_test_update\n",
    "dec_tr_predict = dec_tr_model.predict(X_test_update)\n",
    "#Check accuracy classifications and store it in the accuracy dictionary accuracy_dict\n",
    "accuracy_dict['dec_tr'] = accuracy_score(y_test, dec_tr_predict)\n",
    "#Perform cross-validation\n",
    "dec_tr_score = cross_val_score(dec_tr_model, X_test_update, y_test, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# 4.Linear Support Vector Classification\n",
    "#==============================================================================\n",
    "#Import packages\n",
    "from sklearn.svm import LinearSVC\n",
    "#Create an instance of the model\n",
    "lin_svc_model = LinearSVC()\n",
    "#Fit the model using training data X_train_update and y_train as target values\n",
    "lin_svc_model.fit(X_train_update,y_train)\n",
    "#Predict the class labels for the test data X_test_update\n",
    "lin_svc_predict = lin_svc_model.predict(X_test_update)\n",
    "#Check accuracy classifications and store it in the accuracy dictionary accuracy_dict\n",
    "accuracy_dict['lin_svc'] = accuracy_score(y_test, lin_svc_predict)\n",
    "#Perfrom cross-validation\n",
    "lin_svc_score = cross_val_score(lin_svc_model, X_test_update, y_test, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Milica\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# 5.Logistic Regression Classifier    \n",
    "#==============================================================================\n",
    "#Import packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Create an instance of the model\n",
    "log_reg_model = LogisticRegression() \n",
    "#Fit the model using training data X_train_update and y_train as target values\n",
    "log_reg_model.fit(X_train_update,y_train)\n",
    "#Predict the class labels for the test data X_test_update\n",
    "log_reg_predict = log_reg_model.predict(X_test_update)\n",
    "#Check accuracy classifications and store it in the accuracy dictionary accuracy_dict\n",
    "accuracy_dict['log_reg']= accuracy_score(y_test, log_reg_predict) \n",
    "#Perfrom cross-validation\n",
    "log_reg_score = cross_val_score(log_reg_model, X_test_update, y_test, cv = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn 0.55835402625\n",
      "naive_bay 0.107484923732\n",
      "dec_tr 0.639233770841\n",
      "lin_svc 0.405462930117\n",
      "log_reg 0.405462930117\n",
      "                 model      cv_10\n",
      "0       KNN Classifier   0.535835\n",
      "1       Naive Bayesian  0.0281716\n",
      "2        Decision Tree   0.702991\n",
      "3           Linear SVM  0.0825071\n",
      "4  Logistic Regression   0.281156\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "# Evaluate classifiers by observing their accuracy (accuracy_dict) and cross-validation (score_table)\n",
    "#==============================================================================\n",
    "#Create dataframe for evaluating a score by cross-validation \n",
    "score_table = pd.DataFrame(columns = ['model', 'cv_10'])\n",
    "#Define a list of evaluated models\n",
    "models = ['KNN Classifier', 'Naive Bayesian', 'Decision Tree','Linear SVM','Logistic Regression']\n",
    "#Insert a mean value of each model\n",
    "score_list = [knn_score.mean(),naive_bay_score.mean(),dec_tr_score.mean(),lin_svc_score.mean(),log_reg_score.mean()]\n",
    "\n",
    "for model, n, score in zip(models, np.arange(len(models)), score_list):\n",
    "    score_table.loc[n,'model'] = model\n",
    "    score_table.loc[n,'cv_10'] = score \n",
    "    \n",
    "#Print values from accuracy_dict    \n",
    "for i in accuracy_dict:\n",
    "    print (i, accuracy_dict[i])  \n",
    "    \n",
    "#Evaluating the estimator performance using cross validation\n",
    "print (score_table)    \n",
    "    \n",
    "#By observing the accuracy score and cross-validatio score of the techniques it can be seen that technique dec_tr provides the best result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
